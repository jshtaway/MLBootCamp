{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Random Variables and Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we talked about **discrete** random variables, which attain a discrete (i.e. separated) set of values.\n",
    "\n",
    "### Two ways to see them:\n",
    "- As values on a big Probability space (universe-centric)\n",
    "- Via their Probability distributions (value-centric)\n",
    "\n",
    "Today we'll focus on **probability distributions** as a way of understanding random variables!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability Density Functions (PDFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a discrete random variable, we have a picture for the probability of each of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However these are usually normalized so that the **probability** that we attain a value is the **area** in each bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **continuous** random variable $X$ is one that attains a continuum of values.  These can be described as having a probability density function $\\rho_X(x)$ tells us the **probability density** at each value.  \n",
    "\n",
    "This means if we look at a small interval $[x, x+dx]$ around $x$ and ask about the probability that the continuous random variable $X$ takes values in there, we have that \n",
    "\n",
    "$$\n",
    "P(x \\leq X \\leq x + dx) \\approx \\rho_X(x) dx\n",
    "$$\n",
    "\n",
    "**Warning:** This means that $\\rho_X(x)$ doesn't tell us the probability $P(X = x)$ of the random variable $X$ attaining a specific value $x$ -- this is (almost) always zero!  Instead we must ask about an **interval** of values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some great examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform random variables\n",
    "\n",
    "A **uniform random variable** on the interval $[a,b]$ is any random variable $X$ with a probability density function satisfying\n",
    "$$ $$\n",
    "$$\n",
    "\\rho_X(x) = \\begin{cases}\n",
    "\\frac{1}{b-a} &\\qquad \\text{if $a \\leq x \\leq b$,} \\\\\n",
    "0 & \\qquad \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<img width=400 \n",
    "src=\"https://upload.wikimedia.org/wikipedia/commons/9/96/Uniform_Distribution_PDF_SVG.svg\">\n",
    "\n",
    "This is the most common distribution that vanishes outside of a closed interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian / Normal Distribution\n",
    "\n",
    "A **normal random variable** $\\mathcal{N}(\\mu, \\sigma^2)$ with mean $\\mu$ and variance $\\sigma^2>0$ is any random variable $X$ with a probability density function satisfying\n",
    "$$ $$\n",
    "$$\n",
    "\\rho_X(x) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^\\frac{-(x-\\mu)^2}{2\\sigma^2} = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left({\\frac{-(x-\\mu)^2}{2\\sigma^2}}\\right).\n",
    "$$\n",
    "\n",
    "<img width=400  src=\"https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg\">\n",
    "\n",
    "This is really important -- and describes a certain kind of \"random\" noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-Gaussian\n",
    "A **log-normal random variable** with \"mean\" $\\mu$ and \"variance\" $\\sigma^2 > 0$ is any random variable $X$ with a probability density function satisfying\n",
    "$$ $$\n",
    "$$\n",
    "\\rho_X(x) \n",
    "= \\begin{cases}\n",
    "\\frac{1}{\\sqrt{2\\pi}x \\sigma} \n",
    "e^\\frac{-(\\ln(x)-\\mu)^2}{2\\sigma^2} & \\qquad \\text{if $x > 0$,}\n",
    "\\\\\n",
    "0 & \\qquad \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<img width=\"400\" height=\"400\"\n",
    "src=\"https://upload.wikimedia.org/wikipedia/commons/a/ae/PDF-log_normal_distributions.svg\">\n",
    "\n",
    "This is a common non-negative random variable, that comes up when we have \"mutliplicative noise\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exponential distribution\n",
    "An **exponential random variable** with parameter $\\lambda > 0$ is any random variable $X$ with a probability density function satisfying\n",
    "$$ $$\n",
    "$$\n",
    "\\rho_X(x) = \n",
    "\\begin{cases}\n",
    "λ e^{−λx} = \\lambda \\exp(-\\lambda x) &\\qquad \\text{if $x \\geq 0$,} \\\\\n",
    "0 & \\qquad \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<img width=\"400\" height=\"400\"\n",
    "src=\"https://upload.wikimedia.org/wikipedia/commons/e/ec/Exponential_pdf.svg\">\n",
    "\n",
    "Here $\\lambda$ is called the **rate parameter**, and this is often used to describe the time between occurences of events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Poisson distribution -- a useful discrete random variable\n",
    "An **Poisson random variable** with parameter $\\lambda > 0$ is any random variable $X$ with a probability density function satisfying\n",
    "$$ $$\n",
    "$$\n",
    "\\rho_X(k) = \\begin{cases}\n",
    "\\frac{\\lambda^k}{e^\\lambda \\cdot k!} &\\qquad \\text{if $k$ is a non-negative integer,} \\\\\n",
    "0 & \\qquad \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "<img width=\"400\" height=\"400\"\n",
    "src=\"https://upload.wikimedia.org/wikipedia/commons/1/16/Poisson_pmf.svg\">\n",
    "\n",
    "\n",
    "This is often used to describe the number of events that occur in a given amount of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent random variables\n",
    "\n",
    "Two random varaiables $X$ and $Y$ are called **independent** if for any two intervals $I_1$ and $I_2$ we have that \n",
    "$$ $$\n",
    "$$\n",
    "P(X \\in I_1 \\text{ and } Y \\in I_2) \n",
    "= \\int_{x \\in I_1} \\int_{y \\in I_2} \\rho_X(x) \\rho_Y(y)\\, dx\\, dy.\n",
    "$$\n",
    "\n",
    "In other words the random variables $X$ and $Y$ are independent if their **joint probability density** $\\rho_{X,Y}(x,y)$ has the form\n",
    "$$ $$\n",
    "$$\\rho_{X,Y}(x,y) =\\rho_X(x) \\rho_Y(y).$$\n",
    "\n",
    "Otherwise we say that $X$ and $Y$ are **dependent**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful quantities\n",
    "\n",
    "- **Expected Value / Mean**\n",
    "$$ $$\n",
    "$$ E(X) := \\int_{x \\in \\mathbb{R}} x \\cdot \\rho_X(x) \\, dx $$\n",
    "    \n",
    "- **Variance**\n",
    "$$ $$\n",
    "\\begin{align} \n",
    "Var(X) := (\\sigma_x)^2 &:= E\\left((X - E(X))^2\\right) \n",
    "\\\\\n",
    "&= \\int_{x \\in \\mathbb{R}} (x - E(X))^2 \\cdot \\rho_X(x) \\, dx \\geq 0\n",
    "\\end{align}\n",
    "\n",
    "- **Standard Deviation**\n",
    "$$ $$\n",
    "$$\\sigma_x := \\sqrt{Var(X)} \\geq 0$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transition from discrete to continuous random variables is essentially to replace\n",
    "$$ $$\n",
    "$$\\sum_\\text{values $x$ of $X$} P(X=x) \\times (\\text{Blah})$$ \n",
    "with \n",
    "$$ $$\n",
    "$$\\int_{x\\in \\mathbb{R}} (\\text{Blah}) \\cdot \\rho_X(x)\\, dx$$\n",
    "in all of the formulas!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For two random variables $X$ and $Y$, their **covariance** is given by the formula\n",
    "$$ $$\n",
    "$$\n",
    "\\int_{x \\in \\mathbb{R}} \\int_{y \\in \\mathbb{R}}\n",
    "(x - E(X))(y - E(Y)) \\cdot \\rho_{X\\times Y}(x,y) \\, dy \\, dx\n",
    "$$\n",
    "$$ $$\n",
    "which can also be written more simply as \n",
    "$$ $$\n",
    "$$\n",
    "E\\left((X - E(X))(Y - E(Y))\\right).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another important theorem in Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last time we talked about the **Law of Large numbers**, which gave tangible meaning \n",
    "to the expected value $E(X)$ of a random variable $X$ as the **\"expected average value\"** \n",
    "of many independent trials for choosing $X$.\n",
    "\n",
    "Another important theorem is the **Central Limit Theorem**, which gives a similar tangible meaning to the variance $(\\sigma_X)^2$ of a random variable $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Central Limit Theorem (Informal):** Suppose that $X$ is a random variable with mean zero and finite variance.  Then the sum of $n$ trials of $X$ divided by $\\sqrt{n}$ approaches the normal distribution with mean zero and the same variance as $X$.\n",
    "\n",
    "**Central Limit Theorem (Formal):** Suppose that $X$ is a random variable with $E(X) = 0$ and variance $\\sigma^2 < \\infty$, and $X_1, X_2, \\cdots, X_n$ are independent random variables with the same probability distribution as $X$.  Then the limit\n",
    "$$ $$\n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\frac{X_1 + X_2 + \\cdots + X_n}{\\sqrt{n}} \n",
    "= \\mathcal{N}\\left(0, \\sigma^2\\right)\n",
    "$$\n",
    "in probability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Moral Point of CLT:** If we have a lot of small independent things happening with no bias, then their cumulative effect will be given by something very close to a normal distribution!\n",
    "\n",
    "$$ $$\n",
    "**Other Comments about the CLT:**\n",
    "- It tells us that normal distributions $\\mathcal{N}(\\mu, \\sigma^2)$ are really important!\n",
    "- We can almost see this from our previous numerical examples with LLN. (Look!)\n",
    "- It implies the Law of Large Numbers (just shift to mean zero and divide by $\\sqrt{n}$ again).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Common Confusions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The sum of two random variables sums their outcomes, not their probability distributions!  (Look at the sum of two dice to see this!)\n",
    "$$ $$\n",
    "- Not all random variables are independent!\n",
    "$$ $$\n",
    "- We've been assuming that a nice probabiity density function $\\rho_X(x)$ exists, and that the mean $E(X)$ and variance $(\\sigma_X)^2$ of $X$ exist -- this is not always true and leads to lots of technical difficulties/assumptions in studying Probability!  It's very ok to assume this most of the time in practice!\n",
    "$$ $$\n",
    "- Expected values are your friend, Variances are nice, but probability densities really help you to see what's going on.  Don't be afraid to look! =)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
