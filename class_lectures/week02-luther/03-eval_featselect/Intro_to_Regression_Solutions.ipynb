{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Regression\n",
    "\n",
    "We're going to go through a few regression exercises that will help you better see how regression might work in action in Python.  \n",
    "\n",
    "## Learning Goals\n",
    "The packages and techniques we'll hope to get comfortable with in this activity are the following:\n",
    "- [***statsmodels***](http://statsmodels.sourceforge.net/): a package mainly best at doing regressions with traditional R formula syntax\n",
    "- [***R formulas***](http://science.nature.nps.gov/im/datamgmt/statistics/r/formulas/): R formulas are a convenient way for encapsulating functional relationships for regressions\n",
    "- [***seaborn***](http://stanford.edu/~mwaskom/software/seaborn/): We'll use seaborn for **visualization** as we go along\n",
    "- [***scikit-learn***](http://scikit-learn.org/dev/index.html): This is the main machine learning package we'll be using throughout the course.  It has a multitude of machine learning algorithms and helpful machine learning pipeline tools.  sklearn has a tremendous amount of functionality, to get the most out of this course it will help to really explore the depth of the documentation on your own and watch as you understand more and more of the functionality as the course progresses.\n",
    "- [***Linear Regression***](http://scikit-learn.org/stable/modules/linear_model.html) with scikit-learn:  We'll work with the basic linear regression module(s)\n",
    "- [***Variable Preprocessing and Polynomial Regression***](http://scikit-learn.org/dev/modules/preprocessing.html#preprocessing) with scikit-learn:  We'll be **\"standardizing\"** or **\"normalizing\"** many of our variables to yield better model data.  We'll show how the \"linear\" models can be extended to basically any type of function by using functions of the different fields as the inputs to the linear model.\n",
    "- [***Regularization and Cross-Validation***](http://scikit-learn.org/dev/model_selection.html#model-selection): This is how you know if your model will **generalize** to real-world data and how you evaluate the success of your model.  We will see more on **cross-validation** to come so we'll take it as given today.  Today, the focus is on **regularization**.\n",
    "\n",
    "## Datasets\n",
    "We'll take a look at a few different datasets:\n",
    "1. [Survey Responses](http://www.ats.ucla.edu/stat/examples/chp/p054.txt) (done together)\n",
    "2. Manufactured random dataset  (done together)\n",
    "3. [Bike sharing dataset](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset) (exercise, in pairs)\n",
    "\n",
    "## Setup\n",
    "Try running the following imports and check that they all import successfully.  If they do not, the commands for the installs are given below.  If necessary, at a command line window use `pip` to install the ones that are failing for you and then retry the imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Python 2 & 3 Compatibility\n",
    "from __future__ import print_function, division\n",
    "\n",
    "# Necessary imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import patsy\n",
    "import seaborn as sns\n",
    "from seaborn import plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installations (if necessary)\n",
    "<pre><code>pip install pandas</code></pre>\n",
    "<pre><code>pip install numpy</code></pre>\n",
    "<pre><code>pip install statsmodels</code></pre>\n",
    "<pre><code>pip install seaborn</code></pre>\n",
    "<pre><code>pip install scikit-learn</code></pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survey Data\n",
    "We will use this [simple survey data](http://www.ats.ucla.edu/stat/examples/chp/p054.txt) to demonstrate a few basic features of ***statsmodels*** and ***seaborn*** and how they might be used in a data science workflow for regression.\n",
    "\n",
    "The dataset is simply the results of a survey where the question responses are all numeric.  This leads to 6 numeric independent variable (predictor) fields and 1 numeric dependent variable (response) field.  The predictors are labeled ***X<sub>i</sub>*** and the response is labeled ***Y***.\n",
    "\n",
    "Let's load the dataset in using ***pandas*** and take a look at it.  Here we use [***pandas.read_table***](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_table.html) to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data in\n",
    "df = pd.read_table('http://www.ats.ucla.edu/stat/examples/chp/p054.txt')\n",
    "# Take a look at the datatypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take a look at the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the column names, we'll notice we have the trailing whitespace problem again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can remove this by calling map on the columns list and stripping the whitespace with strip.  The ***map*** function is applied to Series objects, whereas the ***apply*** and ***applymap*** functions are called on Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns = df.columns.map(str.strip)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How many rows and columns does the dataset have?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing with Seaborn\n",
    "We see that the data has 30 responses with 7 fields (6 independent, 1 dependent) each.  Let's use pandas to check out the correlations between the different variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# View the correlations\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation and Multicollinearity\n",
    "We notice that some of the variables are highly correlated.  This is something we might want to take into consideration later.  When 2 predictor variables are highly correlated this is called [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity) and it is something we want to watch out for as it can destabilize our model.  In the extreme case, when 2 predictors are perfectly correlated then there is absolutely nothing gained by making both variables part of our regression.\n",
    "\n",
    "The other takeaway from this table is that some of our predictors are highly correlated with our ***target variable Y***.  This is a good thing, it means that these are the variables that we most likely want to include as part of our model as they explain a large amount of the variance in the target variable (correlation=R, variance_explained=R<sup>2</sup>).\n",
    "\n",
    "Let's try to visualize these correlations all together by using the [***seaborn pairplot***](http://stanford.edu/~mwaskom/software/seaborn/generated/seaborn.pairplot.html) function.  What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot all of the variable-to-variable relations as scatterplots\n",
    "sns.pairplot(df, size = 1.2, aspect=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares Regression with Statsmodels\n",
    "Now that we have a feel for our data, let's jump right in and try a basic regression model.  \n",
    "\n",
    "#### Statsmodels\n",
    "We are going to use the [**`statsmodels`**](http://statsmodels.sourceforge.net/) library first.  `statsmodels` is a Python package for implementing [**linear models**](https://en.wikipedia.org/wiki/Linear_model), of which **Linear Regression** is one.  It has a bunch of nice features for evaluating and executing such models.  Essentially, a linear model is one that is a **linear function of the parameters**.  For Linear Regression, this means the parameter $\\beta$ (note that here $\\beta$ is a **vector** of parameters, which includes $\\beta_0$, $\\beta_1$, $\\beta_2$, etc).  We'll discuss linear models generally later but for  now just accept that Linear Regression is one of these.\n",
    "\n",
    "#### Modeling with Statsmodels\n",
    "There are 2 main ways you can generate models with stats models:\n",
    "- Via the `statsmodels.api` package\n",
    "- Via the `statsmodels.formula.api` package\n",
    "\n",
    "For both approaches, you'll need somewhere to use the [R formula](http://science.nature.nps.gov/im/datamgmt/statistics/r/formulas/) styles formulas for defining the relationship between target variable and feature variables in your model.  ***Statsmodels*** uses [***patsy***](http://patsy.readthedocs.org/en/latest/) to convert this syntax into the proper data matrices for input into its linear models under the covers.  There are a variety of interactions and functions of variables that you can incorporate with this syntax, so feel free to check out the docs.\n",
    "\n",
    "Here we'll just start by defining a regression model that takes as its inputs each of the 6 predictor variables.  The other parameter of course is the data that the model is to be built from, our pandas dataframe.\n",
    "\n",
    "This first model fitting is done for you, it fits a multiple linear regression model of the following form (notice the use of [MathJax](https://www.mathjax.org/) for rendering such lovely math equations in markdown):\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\beta_3X_3 + \\beta_4X_4 + \\beta_5X_5 + \\beta_6X_6\n",
    "$$\n",
    "\n",
    "##### `statsmodels.api`\n",
    "To use this method, you need to generate a **matrix** of **features**, **`X`** and a **vector** of **targets**, **`y`** where each row represents a single **observation**.  In statsmodels, you can do this with a call to **`patsy.dmatrices`**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create your feature matrix (X) and target vector (y)\n",
    "y, X = patsy.dmatrices('Y ~ X1 + X2 + X3 + X4 + X5 + X6', data=df, return_type=\"dataframe\")\n",
    "# Create your model\n",
    "model = sm.OLS(y, X)\n",
    "# Fit your model to your training set\n",
    "fit = model.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `statsmodels.formula.api`\n",
    "The formula approach handles the creation of the `X` and `y` matrices internally, so all you have to do is supply the R formula for your model when you create your `ols` object.  \n",
    "\n",
    "**NOTE:** We'll use this for the remainder of these exercises, but many people like to stick with the `dmatrices` and `X`, `y` matrix creation approach because that is the way `sklearn` works (see later).  Notice that here we've built the same model and it's deterministic, so the results are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "lm1 = smf.ols('Y ~ X1 + X2 + X3 + X4 + X5 + X6', data=df)\n",
    "# Fit the model\n",
    "fit1 = lm1.fit()\n",
    "# Print summary statistics of the model's performance\n",
    "fit1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression Statistics\n",
    "From this we get a handful of useful statistics describing our regression.  A quick google search on any should yield details for those interested.  We will focus on the ***R<sup>2</sup>*** values and the middle table here.\n",
    "\n",
    "***R<sup>2</sup>*** is the square of the correlation coefficient and represents the estimated percentage of the variance in our target variable ***Y*** that can be explained by our regression model.  ***Adjusted R<sup>2</sup>*** also penalizes for things such as large coefficients and extra variables to try and limit ***overfitting*** so it is often a better measure of model efficacy.  We will use that here.\n",
    "\n",
    "The middle table provides the **coefficients** that are regression has found, along with the **standard error** for each coefficient.  This defines our model, aka these are the model parameters that our algorithm was seeking to determine.  \n",
    "\n",
    "The **t-scores** are values that the coefficients score in the [Student's T Distribution](https://en.wikipedia.org/wiki/Student's_t-distribution) and the **P(|t|)** field represents the probability of finding such a t-score if the actual value of the coefficient were 0.  In other words, if we had a coefficient whose true value should be 0 (aka the predictor has no impact on the response) then this P-value is the probability of finding such a coefficient value in our regression by random chance.  In essence, it measures our degree of belief that the coefficient for each variable should be zero.  Thus, the lowest P-values represent the most likely predictors to be impacting the response.\n",
    "\n",
    "Putting it all together, the final column returns a **95% Confidence Interval** for the value of each coefficient.\n",
    "\n",
    "Given these stats, lets remove the highest 3 P-values from our regression model, from ***X<sub>2</sub>***, ***X<sub>4</sub>***, and ***X<sub>5</sub>*** and see how our model performs now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Define the model removing X2, X4, and X5\n",
    "lm2 = smf.ols('Y ~ X1 + X3 + X6', data=df)\n",
    "# Fit the model\n",
    "fit2 = lm2.fit()\n",
    "# Check out the results\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see our **Adjusted R<sup>2</sup>** has increased, and our P-values are lower so we likely have a better model.  Let's just try removing ***X<sub>6</sub>*** to see if that might improve our model a little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Define the model removing X6 this time\n",
    "lm3 = smf.ols('Y ~ X1 + X3', data=df)\n",
    "# Fit the model\n",
    "fit3 = lm3.fit()\n",
    "# Check out the results\n",
    "fit3.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, both **R<sup>2</sup>** coefficients decreased so lets stick with the 2nd model.\n",
    "\n",
    "### Plotting Residuals\n",
    "Before we call it a day with this model and dataset, let's take a quick look at a plot of our residuals *(actual value - predicted value)* with this model.  We do this because in a good model we essentially want our errors to be random.  If our residuals look systematic (e.g. missing high for one range and low for another) then we probably are missing the actual functional dependency underlying the data (perhaps it's not really linear).\n",
    "\n",
    "Take a look [here]([a bad residue plot](http://blog.minitab.com/blog/adventures-in-statistics/regression-analysis-how-do-i-interpret-r-squared-and-assess-the-goodness-of-fit) for an example of a bad residual plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use statsmodels to plot the residuals\n",
    "fit2.resid.plot(style='o', figsize=(12,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope this looks pretty random to me!  So let's move on to some more interesting modeling functions with sklearn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and Polynomial Regression with sklearn\n",
    "Statsmodels has decent functionality for linear models, but scikit-learn has a massive amount of modeling options for all sorts of algorithms as well as data preparation and is growing every day, so we will generally be working with that from here on.\n",
    "\n",
    "### Regression with sklearn\n",
    "Before we jump into some of the additional features of sklearn, let's try to repeat our basic survey example using sklearn's built in **LinearRegression**.\n",
    "\n",
    "You should still have your Dataframe loaded from earlier.  Let's try repeating some of the different models we tried earlier with sklearn.  Here's the first model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an empty model\n",
    "lr = LinearRegression()\n",
    "# Choose the predictor variables, here all but the first which is the response variable\n",
    "# This model is analogous to the Y ~ X1 + X2 + X3 + X4 + X5 + X6 model\n",
    "X = df.iloc[:, 1:]\n",
    "# Choose the response variable(s)\n",
    "y = df.iloc[:, 0]\n",
    "# Fit the model to the full dataset\n",
    "lr.fit(X, y)\n",
    "# Print out the R^2 for the model against the full dataset\n",
    "lr.score(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look back you'll notice that this is the same **R<sup>2</sup>** value that was reported for the first model above.  Let's quickly run the best model from earlier (***X1***, ***X3***, and ***X6***) and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Create an empty model\n",
    "lr1 = LinearRegression()\n",
    "# Choose the predictor variables, here all but the first which is the response variable\n",
    "# This model is analogous to the Y ~ X1 + X3 + X6 model\n",
    "X = df[['X1', 'X3', 'X6']]\n",
    "# Choose the response variable(s)\n",
    "y = df['Y']\n",
    "# Fit the model to the full dataset\n",
    "lr1.fit(X, y)\n",
    "# Print out the R^2 for the model against the full dataset\n",
    "lr1.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the **R<sup>2</sup>** value is the same again.  It's slightly lower, the same as we saw earlier, but the **Adjusted R<sup>2</sup>** value that we saw earlier was higher for this one.  \n",
    "\n",
    "So we've seen how can do simple models with statsmodels and sklearn, but what does anyone see an issue with what we've done here?  What data have we fit and scored our models on?  What claims can we make about the probable performance of our model going forward?\n",
    "\n",
    "### sklearn: What's in a model?\n",
    "Essentially all models in `sklearn` inherit from the same type of \"estimator\" interface.  This means they'll share common methods that we'll see over and over again like:\n",
    "- `fit()`: Fit a model to a set of training data\n",
    "- `score()`: Score the performance of a model on a given sample of data with known ground truth dependent variables\n",
    "- `predict()`: Predict target/response variables based on a sample of independent variables (features, predictors, etc)\n",
    "\n",
    "Additionally, models are usually loaded with other goodies once they've been fit, which can provide information about the resulting fitted model.  For instance, the following might be of interest in regression:\n",
    "- `intercept_`: our $\\beta_0$ intercept in our regression model\n",
    "- `coef_`: the other $\\beta$s in our model\n",
    "\n",
    "Let's print those out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# print out intercept\n",
    "print(lr.intercept_)\n",
    "# print out other coefficients\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pickling for Later\n",
    "We're going to come back to this dataset!  So let's pickle it away for later...\n",
    "\n",
    "`sklearn`, `pandas` and `statsmodels` have their own respective methods for \"pickling\" their objects.\n",
    "\n",
    "**Pandas:**\n",
    "\n",
    "To pickle a `pandas.DataFrame` use the [`to_pickle()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_pickle.html) method.  Use this now to pickle your training data `df` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle your pandas dataframe\n",
    "df.to_pickle('data/survey_data.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Statsmodels**\n",
    "\n",
    "To pickle a statsmodels object us the [`save()`](http://statsmodels.sourceforge.net/devel/generated/statsmodels.regression.linear_model.OLSResults.save.html#statsmodels.regression.linear_model.OLSResults.save) method.  Use this to pickle your best fit model `fit2` to `survey_sm_model.pkl` now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle fit2 to a file \n",
    "fit2.save('data/survey_sm_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**sklearn**\n",
    "\n",
    "Pickle an `sklearn` model to the file `survey_sk_model.pkl` using the `sklearn` replacement for pickle `joblib` as seen [here](http://scikit-learn.org/stable/modules/model_persistence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(lr, 'data/survey_sk_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "So far we've only tried to create regression models that are linear functions of the predictor variables.  However, there's no reason we can't transform the predictor variables by any type of function we want before inputting them to linear regression.  This is the idea behind [**Polynomial Regression**](https://en.wikipedia.org/wiki/Polynomial_regression) and it allows us (along with similar functional regressions) to essentially model our response variables as any function of our predictor variables that we like.  Viewed in this way, Linear Regression is just a special instance of Polynomial Regression with a polynomial of degree 1.\n",
    "\n",
    "#### Polynomial Regression with sklearn\n",
    "sklearn has built-in options for converting your predictor variables to polynomial functions of them.  In this exercise we'll use the [**PolynomialFeatures**](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) class of sklearn to manipulate incoming predictors into nth-order polynomials of those features.  We'll combine this with the [***make_pipeline***](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html) function to string together a pipeline of operations that is able to first transform our linear features into polynomial features and then run a linear regression against the resulting polynomial features. \n",
    "\n",
    "##### Generating Random Data\n",
    "The first thing we're going to do is manufacture some data from a known distribution with a little additive noise.  This allows us to compare our results to the known ground truth.  Let's create that data from a sine curve as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "figsize(5,5)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# We start by seeding the random number generator so that everyone will have the same \"random\" results\n",
    "np.random.seed(9)\n",
    "\n",
    "# Function that returns the sin(2*pi*x)\n",
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "# generate points used to plot\n",
    "# This returns 100 evenly spaced numbers from 0 to 1\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "n_samples = 100\n",
    "# Generate the x values from the random uniform distribution between 0 and 1\n",
    "X = np.random.uniform(0, 1, size=n_samples)[:, np.newaxis]\n",
    "# Generate the y values by taking the sin and adding a random Gaussian (normal) noise term\n",
    "y = f(X) + np.random.normal(scale=0.3, size=n_samples)[:, np.newaxis]\n",
    "\n",
    "# Plot the training data against what we know to be the ground truth sin function\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
    "ax.scatter(X, y, label='data', s=100)\n",
    "ax.set_ylim((-2, 2))\n",
    "ax.set_xlim((0, 1))\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting an nth-degree Polynomial\n",
    "Now that we have our data and know the ground truth, let's try fitting a 3rd degree polynomial to our training data and see how it looks.  3rd degree makes sense for this interval because the sin function has 2 turning points over the interval [0,1] and a 3rd degree polynomial will general have 2 (or less) turning points.\n",
    "\n",
    "We first define a function `plot_approximation` that takes a pipeline of steps from make_pipeline and some plotting info and will plot the results of the sklearn pipeline on the specified plot with the ground truth and data in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import PolynomialFeatures and make_pipeline for Polynomial Regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Plot the results of a pipeline against ground truth and actual data\n",
    "def plot_approximation(est, ax, label=None):\n",
    "    \"\"\"Plot the approximation of ``est`` on axis ``ax``. \"\"\"\n",
    "    ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
    "    ax.scatter(X, y, s=100)\n",
    "    ax.plot(x_plot, est.predict(x_plot[:, np.newaxis]), color='red', label=label)\n",
    "    ax.set_ylim((-2, 2))\n",
    "    ax.set_xlim((0, 1))\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.legend(loc='upper right',frameon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate our pipeline for a 3rd degree polynomial and try it out in our plotting function.  Note that the steps are:\n",
    "- Use PolynomialFeatures(3) to create a generator of 3rd degree polynomials\n",
    "- Feed this generator to make_pipeline along with a LinearRegression object to tell it to string together these operations when given a new set of input predictor variables.  This results in a new model object that has the same `fit()`, `score()`, `predict()`, etc functions\n",
    "- Call `fit()` on our new object to fit a 3rd degree polynomial regression\n",
    "- Send the result to our plotting function to view the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "fig,ax = plt.subplots(1,1)\n",
    "# Set the degree of our polynomial\n",
    "degree = 3\n",
    "# Generate the model type with make_pipeline\n",
    "# This tells it the first step is to generate 3rd degree polynomial features in the input features and then run\n",
    "# a linear regression on the resulting features\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# Fit our model to the training data\n",
    "est.fit(X, y)\n",
    "# Plot the results\n",
    "plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: Qualitatively**, how would you characterize this fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fit of a polynomial of degree 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "fig,ax = plt.subplots(1,1)\n",
    "# Set the degree of our polynomial\n",
    "degree = 2\n",
    "# Generate the model type with make_pipeline\n",
    "# This tells it the first step is to generate 3rd degree polynomial features in the input features and then run\n",
    "# a linear regression on the resulting features\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# Fit our model to the training data\n",
    "est.fit(X, y)\n",
    "# Plot the results\n",
    "plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the fit of a polynomial of degree 9, or heck, how bout 27!?:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up the plot\n",
    "fig,ax = plt.subplots(1,1)\n",
    "# Set the degree of our polynomial\n",
    "degree = 27\n",
    "# Generate the model type with make_pipeline\n",
    "# This tells it the first step is to generate 3rd degree polynomial features in the input features and then run\n",
    "# a linear regression on the resulting features\n",
    "est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "# Fit our model to the training data\n",
    "est.fit(X, y)\n",
    "# Plot the results\n",
    "plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What happens as we increase the degree of polynomial?**\n",
    "\n",
    "**Q: Which polynomial should we choose?**\n",
    "\n",
    "To gain some insight into this, let's plot polynomials from degree 1 to 9 and examine how the errors in our predictions change vs. the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step through degrees from 0 to 9 and store the training and test (generalization) error.\n",
    "# This sets up 5 rows of 2 plots each (KEEP)\n",
    "fig, ax_rows = plt.subplots(5, 2, figsize=(15, 20))\n",
    "for degree in range(10):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X, y)\n",
    "    # This sets the appropriate axis for each degree (KEEP)\n",
    "    ax_row_left, ax_row_right = ax_rows[degree/2]\n",
    "    if degree%2 == 0:\n",
    "        ax = ax_row_left\n",
    "    else:\n",
    "        ax = ax_row_right\n",
    "    plot_approximation(est, ax, label='degree=%d' % degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice?**\n",
    "\n",
    "##### Pickling for later\n",
    "We may return to this fake data for further exploration later, so let's pickle our `X` and `y` so we can do just that.  To save `numpy` arrays, you use the [`numpy.save()`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.save.html) method.  We can save multiple arrays to one file with the [`numpy.savez()`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.savez.html) method.  Use this now to save `X` and `y` to the file `poly_data.npz`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pickle the numpy arrays X and y\n",
    "np.savez('data/poly_data.npz', X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized and Cross-Validated Regression\n",
    "As you probably guessed, the problem with our model above was that we trained and tested our model on the same dataset.  This means our model could be very likely to **overfit** and not perform as well when it tries to **generalize** to real world data, and after all ***generalization is the key to machine learning***.\n",
    "\n",
    "Thus, we have a need for **cross-validation** in our model evaluation process.  That is, we need to find a way to train our model on 1 randomly chosen set of data and evaluate it against a separate random test set.  Thankfully sklearn provides a bevy of built-in ways to perform cross-validation.\n",
    "\n",
    "#### Cross-Validation with sklearn\n",
    "The simplest way to perform cross-validation with an sklearn model is to have it perform a random **train/test split** of the data for you.  It's customary to use something like 2/3 of your data for training, and the remaining 1/3 for testing.  \n",
    "\n",
    "sklearn's [***train_test_split***](http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.train_test_split.html) function provides exactly that.  Here's an example of it in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# INSTRUCTOR NOTE: Run this multiple times to show the variation\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "# Fit the model against the training data\n",
    "lr.fit(X_train, y_train)\n",
    "# Evaluate the model against the testing data\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What have we done?**\n",
    "We've now scored our a trained model against a test set.  How can we see the value of this?  Well let's remind ourselves of the score that we got by scoring against the training set by doing that again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lr1 = LinearRegression()\n",
    "lr1.fit(X, y)\n",
    "lr1.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Whoa!**\n",
    "\n",
    "Now we can begin to see the value of a hold out test set.  Notice that our model performance is significantly decreased on the test set.  This is **okay!**  In fact, this is better as we now have a much more accurate number to report regarding probable model performance in the real world, and we won't be making false promises to our CEO :)\n",
    "\n",
    "There are a variety of ways to do cross-validation, and sklearn's modules for handling this can be found [here](http://scikit-learn.org/dev/modules/cross_validation.html).  We'll be seeing different methods as we continue in the course, but the basic 2-to-1 split is generally a safe quick and dirty fallback."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularized Regression with sklearn\n",
    "As we've discussed, **regularization** is basically a blanket term for any technique that alters a model to prevent **overfitting**.  In linear regression, this is generally done by adding a term to the **Cost function** that is increasing in the absolute value of the coefficients.  The upshot of this is that models with large coefficients are implicitly punished by increasing their cost function, and the underlying mathematics of regression seek to choose the model that minimizes the value of the cost function.\n",
    "\n",
    "There are 2 very common regularization techniques for regression, called **\"Ridge Regression\"** and **\"Lasso Regression\"**\n",
    "\n",
    "##### Ridge Regression\n",
    "[Ridge Regression](http://scikit-learn.org/stable/modules/linear_model.html#ridge-regression) adds the L2 norm (square root of the sum of squares) of the coefficients to the cost function.\n",
    "\n",
    "##### Lasso Regression\n",
    "[Lasso Regression](http://scikit-learn.org/stable/modules/linear_model.html#lasso) adds the L1 norm (sum of absolute values) of the coefficients to the cost function.\n",
    "\n",
    "##### ElasticNet Regression\n",
    "[ElasticNet Regression](http://scikit-learn.org/stable/modules/linear_model.html#elastic-net) simply combines both by adding **both** the **L1** and **L2** norm seeking the best of both worlds.\n",
    "\n",
    "sklearn has specific model types for both of the above regressions which we will see some of below.  Essentially, there is a \"punishment parameter\" alpha that these models try several different values of with cross-validation and then they return the value and model that yields the best generalization.  The beauty of this is that it all (optionally) happens completely under the covers!  We will use **RidgeCV** later in the exercises to perform regression with automatic Regularization and Cross-Validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression Revisited\n",
    "To see how to use Regularized Regression we're going to revisit our generated data which we were using to fit polynomial regressions.\n",
    "\n",
    "#### Reload the Data\n",
    "Let's reload the `X` and `y` numpy arrays as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the file\n",
    "npz = np.load('data/poly_data.npz')\n",
    "# Retrieve each array\n",
    "X = npz['arr_0']\n",
    "y = npz['arr_1']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Overfitting: The need for Regularization\n",
    "Now that we have our data again we're going to train some increasingly complex (increasing degree) polynomials and watch what happens to the training and test errors as complexity increases.  Let's take a look at the data as we did before except this time we'll split into a train and test set first via `train_test_split`.\n",
    "\n",
    "Don't worry about the `sklearn` details of this right now, we'll return to it tomorrow, the **focus is on regularization**.  Essentially, we're training our model against one set and then testing it against some previously unknown set just as we did in The Hipster Game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.pylabtools import figsize\n",
    "figsize(5,5)\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# We start by seeding the random number generator so that everyone will have the same \"random\" results\n",
    "np.random.seed(9)\n",
    "\n",
    "# Function that returns the sin(2*pi*x)\n",
    "def f(x):\n",
    "    return np.sin(2 * np.pi * x)\n",
    "\n",
    "# generate points used to plot\n",
    "# This returns 100 evenly spaced numbers from 0 to 1\n",
    "x_plot = np.linspace(0, 1, 100)\n",
    "\n",
    "# generate points and keep a subset of them\n",
    "n_samples = 100\n",
    "# Generate the x values from the random uniform distribution between 0 and 1\n",
    "X = np.random.uniform(0, 1, size=n_samples)[:, np.newaxis]\n",
    "# Generate the y values by taking the sin and adding a random Gaussian (normal) noise term\n",
    "y = f(X) + np.random.normal(scale=0.3, size=n_samples)[:, np.newaxis]\n",
    "# Split the data into a 20/80 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8)\n",
    "\n",
    "# Plot the training data against what we know to be the ground truth sin function\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.plot(x_plot, f(x_plot), label='ground truth', color='green')\n",
    "ax.scatter(X_train, y_train, label='data', s=100)\n",
    "ax.set_ylim((-2, 2))\n",
    "ax.set_xlim((0, 1))\n",
    "ax.set_ylabel('y')\n",
    "ax.set_xlabel('x')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to see what happens as we increase the degree of our polynomials and fit each one.  The below code will generate a plot of the error as a function of polynomial degree for degree 1-9:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step through degrees from 0 to 9 and store the training and test (generalization) error.\n",
    "train_error = np.empty(10)\n",
    "test_error = np.empty(10)\n",
    "for degree in range(10):\n",
    "    est = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    est.fit(X_train, y_train)\n",
    "    train_error[degree] = mean_squared_error(y_train, est.predict(X_train))\n",
    "    test_error[degree] = mean_squared_error(y_test, est.predict(X_test))\n",
    "\n",
    "# Plot the training and test errors against degree\n",
    "plt.plot(np.arange(10), train_error, color='green', label='train')\n",
    "plt.plot(np.arange(10), test_error, color='red', label='test')\n",
    "plt.ylim((0.0, 1e0))\n",
    "plt.ylabel('log(mean squared error)')\n",
    "plt.xlabel('degree')\n",
    "plt.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What do you notice?**\n",
    "\n",
    "The higher the degree of the polynomial (our proxy for model complexity), the lower the training error. The testing error decreases too, but it eventually reaches its minimum at a degree of three and then starts increasing at a degree of seven. \n",
    "\n",
    "This is a visual demonstration of ***overfitting***: the model is already so complex that it fits the idiosyncrasies of our training data, idiosyncrasies which limit the model's ability to generalize (as measured by the testing error).\n",
    "\n",
    "In the above example, the optimal choice for the degree of the polynomial approximation would be between three and six. So when we get some data, we could fit a bunch of polynomials and then choose the one that minimizes MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Regularized Regression\n",
    "\n",
    "#### Hand picking polynomials is hard work, and data scientists are lazy so....\n",
    "...we would like a method that eliminates the need to manually select the degree of the polynomial: we can add a constraint to our linear regression model that constrains the magnitude of the coefficients in the regression model. This constraint is called the regularization term and the technique is often called shrinkage in the statistical community because it shrinks the coefficients towards zero. In the context of polynomial regression, constraining the magnitude of the regression coefficients effectively is a smoothness assumption: by constraining the L2 norm of the regression coefficients we express our preference for smooth functions rather than wiggly functions.\n",
    "\n",
    "A popular regularized linear regression model is Ridge Regression. This adds the L2 norm of the coefficients to the ordinary least squares objective:\n",
    "\n",
    "  $J(\\boldsymbol\\beta) = \\frac{1}{n}\\sum_{i=0}^n (y_i - \\boldsymbol\\beta^T \\mathbf{x}_i')^2 + \\alpha \\|\\boldsymbol\\beta\\|_2$\n",
    "\n",
    "where $\\boldsymbol\\beta$ is the vector of coefficients including the intercept term and $\\mathbf{x}_i'$ is the i-th feature fector including a dummy feature for the intercept. The L2 norm term is weighted by a regularization parameter ``alpha``: if ``alpha=0`` then you recover the Ordinary Least Squares regression model. The larger the value of ``alpha`` the higher the smoothness constraint.\n",
    "\n",
    "Below you can see the approximation of a [``sklearn.linear_model.Ridge``](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) estimator fitting a polynomial of degree nine for various values of ``alpha`` (left) and the corresponding coefficient loadings (right). The smaller the value of ``alpha`` the higher the magnitude of the coefficients, so the functions we can model can be more and more wiggly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Set up a figure and axes for 8 plots, 2 per row for 4 rows\n",
    "fig, ax_rows = plt.subplots(4, 2, figsize=(15, 20))\n",
    "\n",
    "# A helper function to plot the absolute value of the coefficients on the right-hand column plot\n",
    "def plot_coefficients(est, ax, label=None, yscale='log'):\n",
    "    coef = est.steps[-1][1].coef_.ravel()\n",
    "    if yscale == 'log':\n",
    "        ax.semilogy(np.abs(coef), marker='o', label=label)\n",
    "        ax.set_ylim((1e-1, 1e8))\n",
    "    else:\n",
    "        ax.plot(np.abs(coef), marker='o', label=label)\n",
    "    ax.set_ylabel('abs(coefficient)')\n",
    "    ax.set_xlabel('coefficients')\n",
    "    ax.set_xlim((1, 9))\n",
    "\n",
    "# Try out 4 different values of the RidgeRegression parameter alpha and watch how the resulting models change\n",
    "# With higher values of alpha, more complex (more wiggly) models will be more punished and thus less likely\n",
    "degree = 9\n",
    "alphas = [0.0, 1e-8, 1e-5, 1e-1]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Ridge(alpha=%r) coefficients' % alpha)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now with the LASSO...\n",
    "\n",
    "In the above example we used Ridge Regression, a regularized linear regression technique that puts an [L2 norm](http://mathworld.wolfram.com/L2-Norm.html) penalty on the regression coefficients. As mentioned above, another popular regularization technique is the LASSO, a technique which puts an [L1 norm](http://mathworld.wolfram.com/L1-Norm.html) penalty instead. The difference between the two is that the LASSO leads to sparse solutions, driving most coefficients to zero, whereas Ridge Regression leads to dense solutions, in which most coefficients are non-zero.\n",
    "\n",
    "Let's check out the same process using the LASSO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Create only 2 plot rows, only trying 2 alphas\n",
    "fig, ax_rows = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot the results next to the coefficient values for each of hte 2 alphas\n",
    "degree = 9\n",
    "alphas = [1e-3, 1e-2]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), Lasso(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='Lasso(alpha=%r) coefficients' % alpha, yscale=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that for the Lasso many of the coefficients are zero.  When you choose an sklearn model that encapsulates both Regularization and Cross-Validation (like [**RidgeCV**](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) to be used below for instance) sklearn will choose the best value from several options of ***alpha*** by cross-validating in whatever way you decide.\n",
    "\n",
    "##### And with ElasticNet...\n",
    "Remember this tries to find the best of both worlds, with both L1 and L2 regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Create only 2 plot rows, only trying 2 alphas\n",
    "fig, ax_rows = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Plot the results next to the coefficient values for each of hte 2 alphas\n",
    "degree = 9\n",
    "alphas = [1e-3, 1e-2]\n",
    "for alpha, ax_row in zip(alphas, ax_rows):\n",
    "    ax_left, ax_right = ax_row\n",
    "    est = make_pipeline(PolynomialFeatures(degree), ElasticNet(alpha=alpha))\n",
    "    est.fit(X_train, y_train)\n",
    "    plot_approximation(est, ax_left, label='alpha=%r' % alpha)\n",
    "    plot_coefficients(est, ax_right, label='ElasticNet(alpha=%r) coefficients' % alpha, yscale=None)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this is some sort of hybrid of the look/feel of the Lasso and Ridge!  \n",
    "\n",
    "Congratulations, you understand regularization!\n",
    "\n",
    "**Remember:** regularization is a general concept.  It's any adjustment to the actual modeling step that tries to inherently control for overfitting, most often by augmenting the cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises: Bike Sharing Dataset\n",
    "For these exercises we'll be exploring the Bike Sharing Dataset available [here](http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset).  The goal is to be able to predict the count of bikes being used at any given time given data on the time of day, year, week, weather conditions etc.\n",
    "\n",
    "#### Data Exploration\n",
    "##### Read the data in\n",
    "The field descriptions are available at the link, and the data has been placed on the repo for you.  Make sure the \"hour.csv\" file is somewhere on your local machine, and then initialize the `path_to_data` variable below.  Use pandas `read_csv()` to load the data into a dataframe and then call `head()` to make sure everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Set the root location where the data is stored\n",
    "path_to_data = 'data/hour.csv'\n",
    "# Read in the bike data\n",
    "df = pd.read_csv(path_to_data)\n",
    "# Use head to view the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [`shape`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shape.html#pandas.DataFrame.shape) to check out how many rows and columns the dataframe has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# How many rows and columns do we have?  You should see 17379 x 17\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use [`info()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.info.html#pandas.DataFrame.info) to get a summary of the dataframe and its datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Let's examine the datatypes\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove** the **non-numeric columns** and the **instant column** (not useful for now) as well as the **atemp column** (because it is going to be almost perfectly correlated with temp) and the **casual and registered columns** (because they would make things too easy :)).  Do this by:\n",
    "- Create a list of columns to keep\n",
    "- Select out only those columns from the dataframe and reassign the dataframe to that selection\n",
    "- Use `head()` to make sure everything worked as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Create a list of columns to keep\n",
    "cols = ['season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'hum', 'windspeed', 'cnt']\n",
    "# Select out only those columns\n",
    "df = df[cols]\n",
    "# Use head to review the remaining data, you should now have 12 columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin modeling, use the [`corr()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.corr.html#pandas.DataFrame.corr) function to get a feel for the correlations among the different variables, especially with regard to 'cnt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Check out the dataframe correlations with corr()\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at only the 'cnt' column of the correlations and order it in descending order wih [`sort_values()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sort_values.html#pandas.DataFrame.sort_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Get the correlations with 'cnt' sorted in descending order\n",
    "df.corr()['cnt'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should now have a better feel for which variables might be most valuable for your model.\n",
    "\n",
    "Now use ***seaborn's*** `pairplot()` function to visualize these correlations for the variables ***mnt, season, hr, temp, hum, and cnt***.  Do any of the distributions jump out at you at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "#Let's try visualizing some of these pairwise correlations with seaborn\n",
    "sns.pairplot(df[['mnth', 'season', 'hr', 'temp', 'hum', 'cnt']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modeling with statsmodels\n",
    "Let's try some exploration with statsmodels.  As a first model, try creating an ordinary least squares model with statsmodels by incorporating all of the variables that had at least a .10 absolute value of correlation with cnt above (**HINT:** this should be 7 variables).\n",
    "- Create your model with the `ols()` function with the appropriate **R Formula** syntax and your dataframe\n",
    "- Fit the model\n",
    "- Print the fit summary to check out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Let's jump right in and try a model with statsmodels using all variables above .10 correlation\n",
    "lsm = smf.ols('cnt ~ temp + hr + yr + season + mnth + weathersit + hum', data = df)\n",
    "fit1 = lsm.fit()\n",
    "print(fit1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seaborn for Exploring Distributions\n",
    "Your **R<sup>2</sup>** should be .386, not great.  That means we believe we can explain only about 38.6% of the variance in count with this model.  Let's use seaborn to explore a few of the relationships between the predictors and response in more detail.\n",
    "\n",
    "Use the **seaborn** [`boxplot()`](http://stanford.edu/~mwaskom/software/seaborn-dev/generated/seaborn.boxplot.html) function (you'll need to specify the `x` and `y` parameters properly to create a boxplot of **temp** vs **count**.  **NOTE:** Make sure that the temp values are ordered by increasing temperature!  What are some things you notice about the relationship between temperature and count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Let's visualize some of the different variables against 'cnt'\n",
    "# Temp\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "ax.set_title('Counts by Temperature')\n",
    "# Create a seaborn boxplot of count by temperature ordered by temperature\n",
    "sns.boxplot(x=df['temp'].sort_values(), y=df['cnt'])\n",
    "ax.set_xlabel('Temp')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create similar boxplots for the humidity, month, year, and season variables.  Take note of any relationships that jump out at you from the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Humidity\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "ax.set_title('Counts by Humidity')\n",
    "# Create a seaborn boxplot of counts by humidity ordered by increasing humidity\n",
    "sns.boxplot(x=df['hum'].sort_values(), y=df['cnt'])\n",
    "ax.set_xlabel('Humidity')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Year\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "ax.set_title('Counts by Year')\n",
    "# Create a seaborn boxplot of counts by year ordered by increasing year\n",
    "sns.boxplot(x=df['yr'].sort_values(), y=df['cnt'])\n",
    "ax.set_xlabel('Year')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Month\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "ax.set_title('Counts by Month')\n",
    "# Create a seaborn boxplot of counts by month ordered by increasing month\n",
    "sns.boxplot(x=df['mnth'].sort_values(), y=df['cnt'])\n",
    "ax.set_xlabel('Month')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Season\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "ax.set_title('Counts by Season')\n",
    "# Create a seaborn boxplot of counts by season ordered by increasing season\n",
    "sns.boxplot(x=df['season'].sort_values(), y=df['cnt'])\n",
    "ax.set_xlabel('Season')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Incorporating nonlinear relationships in statsmodels\n",
    "From your plots, you should clearly notice that month and season do not have a linear relationship with count.  Try to create a statsmodels model that incorporates this.\n",
    "- Define the model using the `ols()` function and the proper R formula and data\n",
    "- Use the syntax `var_name^power` in your R formula to add features for mnth^2, mnth^3, mnth^4 and likewise for season.  This is akin to polynomial regression on those variables with a 4th degree polynomial.  Make sure to retain all of the linear variables from your original regression except **exclude weathersit** for this one.\n",
    "- Call the `fit()` function to fit your model\n",
    "- Call the `summary()` function to view how your model did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Define your model with the appropriate R formula and ols()\n",
    "lsm = smf.ols('cnt ~ temp + hr + mnth + mnth^2 + mnth^3 + mnth^4 + yr + season + season^2 + season^3 + season^4 + hum', df)\n",
    "# Call fit on your model\n",
    "fit2 = lsm.fit()\n",
    "# Call summary to print how you did\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a slight improve in your **Adjusted R<sup>2</sup>** but not too much to get excited about.  So let's focus on the **hr** column now.\n",
    "\n",
    "Use **seaborn** again to make a boxplot of counts by hour as you did for the other variables above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Hour\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "ax.set_title('Counts by Hour')\n",
    "# Create a seaborn boxplot of counts by hour ordered by increasing hour\n",
    "sns.boxplot(x=df['hr'].sort_values(), y=df['cnt'])\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that hour is clearly not linearly related to count either.  So try altering your statsmodels model to incorporate a 4th degree relation with hr as well.\n",
    "- Use `ols()` with everything you had before plus the appropriate 4 **hr** terms to define your model\n",
    "- Call `fit()` on your model\n",
    "- Call `summary()` to print how you did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Define your model with the appropriate R formula and ols()\n",
    "lsm = smf.ols('cnt ~ temp + hr + hr^2 + hr^3 + hr^4 + mnth + mnth^2 + mnth^3 + mnth^4 + yr + season + season^2 + season^3 + season^4 + hum', df)\n",
    "# Call fit on your model\n",
    "fit2 = lsm.fit()\n",
    "# Call summary to print how you did\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see your **R<sup>2</sup>** up to .470 now, but this is a very complex model so we could potentially be overfitting.  So let's try to trim the fat, starting with the month variable.  The mnth^3 term should have the highest P-value and thus be a candidate for removal.  Do just that and then re-evaluate your model.\n",
    "- Create a new model with `ols()` that's the same as the last one minues the **mnth^3** term\n",
    "- Use `fit()` to fit your model\n",
    "- Use `summary()` to print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Use ols to create a model the same as the last minus mnth^3\n",
    "lsm = smf.ols('cnt ~ temp + hr + hr^2 + hr^3 + hr^4 + mnth + mnth^2 + mnth^4 + yr + season + season^2 + season^3 + season^4 + hum', df)\n",
    "# Use fit to fit the model\n",
    "fit2 = lsm.fit()\n",
    "# Use summary to see how you did\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well now, if you did it correctly this is nice now.  Almost all of the P-values are essentially 0 except for **season**.  So as one last iteration try removing season from your model.\n",
    "- Use `ols()` to create a model the same as the last one minus the **season** term\n",
    "- Call `fit()` to fit your model\n",
    "- Call `summary()` to print a summary of how you did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Use ols to create the same model minus season\n",
    "lsm = smf.ols('cnt ~ temp + hr + hr^2 + hr^3 + hr^4 + mnth + mnth^2 + mnth^4 + yr + season^2 + season^3 + season^4 + hum', df)\n",
    "# Call fit to fit your model\n",
    "fit2 = lsm.fit()\n",
    "# Call summary to print results\n",
    "fit2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright not bad, we can explain almost half of the data's variance and all of the coefficients claim to be very decidedly nonzero.  But how do we know how it will perform against a test set?\n",
    "\n",
    "Finally we get to move on to sklearn with this dataset.\n",
    "\n",
    "### sklearn on the Bike Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not already done, import sklearn.linear_model.LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a features data matrix `X` by selecting out only the predictor variables (columns 1-11) from your dataframe and a response vector `y` by selecting only the last column (the cnt colum).  Use `iloc` to get these slices from your pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Separate out our predictor variables from our reponse variables\n",
    "# Dump your predictors into X\n",
    "X = df.iloc[:, 0:10]\n",
    "# Dump your target variables (responses) into y\n",
    "y = df.iloc[:, 11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a [`LinearRegression`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object, train it on `X` and `y` by calling `fit()`, and then print out the performance against your training set by calling `score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Let's generate a 70/30 train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What have we done?\n",
    "We've created a model akin to our first model in statsmodels.  It's a completely linear model and performs similarly, but does anyone have any reservations as to how we've defined our performance...?\n",
    "\n",
    "From here we could proceed to try out some of the polynomical models that we used with statsmodels above.  However, if you remember the distribution of the counts by **hours** from above, you might be thinking it could be more fruitful to investigate that variable a little more and in a smarter way.  If so, congratulations!  You're on your way to becoming a good data scientist!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reexamining Hours\n",
    "Use seaborn again to generate the boxplot of counts by hour again, for reference.  Since you've presumably already done this, it has been filled in for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's examine the hours more closely\n",
    "# Hour\n",
    "fig, ax = plt.subplots(1,1, figsize=(20,10))\n",
    "ax.set_title('Counts by Hour')\n",
    "sns.boxplot(x=df['hr'].sort_values(), y=df['cnt'])\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do you notice?\n",
    "The key observation here is (to this author at least), there seems to be 3 distinct classes of traffic during the day:  \n",
    "1. The low-traffic night hours  \n",
    "2. The high-traffic peak commute hours  \n",
    "3. The middle-traffic daytime hours\n",
    "\n",
    "#### What can we do with this observation?\n",
    "##### Indicator (Dummy) Variables\n",
    "As a first attempt, let's try to create [***indicator variables***](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) `hr_low`, `hr_high`, and `hr_med` that represent the 3 traffic situations described above.  Indicator variables are 0/1 binary variables that indicate whether a condition is met or not, and they are quite useful in regression as they have an easy conceptual understanding: a value of 1 for a given field increases the target variable by the amount of its model coefficient.  These are often useful when provided with categorical attributes.  Any field with n unique categorical values can be reformulated into n indicator variable fields where each represents whether or not that attribute value is present.\n",
    "\n",
    "So let's use indicator variables for the hours to classify low, medium, and high traffic times.  To do this:\n",
    "- Generate additional columns of your dataframe called `hr_low`, `hr_med`, and `hr_high` and initialize them to 0\n",
    "- For each column, replace the column with a call to `map()` on the dataframe's **hr** column\n",
    "- This `map()` should call a `lambda` function that returns 1 if the **hr** is in the list of hours values for that particular traffic group (as judged by your own observation of the boxplot above) and 0 otherwise\n",
    "- Call `head(24)` to make sure the results make sense for an entire day's worth of hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Create the 3 new columns initialized to 0\n",
    "# You can do this the same way you might add a new entry to a Dict\n",
    "df['hr_low'] = 0\n",
    "df['hr_med'] = 0\n",
    "df['hr_high'] = 0\n",
    "\n",
    "# Use a map function on each new column to make it a 1 if it should be, 0 otherwise\n",
    "df['hr_low'] = df.hr.map(lambda x: 1 if (x < 7 or x > 20) else 0)\n",
    "df['hr_med'] = df.hr.map(lambda x: 1 if x in [7, 9, 10, 11, 12, 13, 14, 15, 16, 19, 20] else 0)\n",
    "df['hr_high'] = df.hr.map(lambda x: 1 if x in [8, 17, 18] else 0)\n",
    "df.head(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What have we done?\n",
    "We have some new variables in our dataframe now.  Let's take a look at all of the correlations to **cnt** now.\n",
    "- Use `corr()` to retrieve all correlations\n",
    "- Select out only the **cnt** column\n",
    "- Use `sort_values()` to sort them in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# Let's take a look at the correlations of our new variables\n",
    "df.corr()['cnt'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite promising, our new variables are quite strongly correlated with **cnt**.\n",
    "\n",
    "Now, finally let's try tossing out the **hr** column and use our 3 newly defined columns in its place.  Let's use those along with our original 6 other attributes to generate a new `LinearRegression` model with `sklearn`.  \n",
    "- Generate a data matrix `X` by selecting out the fields `hr_low`, `hr_med`, `hr_high`, `temp`, `hum`, `weathersit`, `windspeed`, `season`, `workingday`.  Use `loc` for this.\n",
    "- Generate a target variables vector `y` by selecting out just the `cnt` column.\n",
    "- Use `train_test_split()` to generate `X_train`, `X_test`, `y_train`, `y_test` with a 70/30 split\n",
    "- Fit your model with `fit()` against `X_train` and `y_train`\n",
    "- Score your model against `X_test` and `y_test` with `score()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is promising, both the hr_high and hr_low have stronger correlations than any from before\n",
    "# Let's try building another model with this new information\n",
    "X = df.loc[:, ['hr_low', 'hr_med', 'hr_high', 'temp', 'hum', 'weathersit', 'windspeed', 'season', 'workingday']]\n",
    "y = df['cnt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happened?\n",
    "Well, this is awesome!  With just a slightly deeper understanding of the hours distribution we've made a marked improvement in our model (you should now see a score around .57).\n",
    "\n",
    "### Automatic Cross-Validation with sklearn\n",
    "So far we've just been using train/test splits for cross-validation and we haven't let sklearn do the work for us.  We also haven't even touched regularization.  So here we're going to do both, and we're going to do it in just 3 lines!\n",
    "\n",
    "First, import sklearn.linear_model.RidgeCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "from sklearn.linear_model import RidgeCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, create a `RidgeCV` object with the parameter `cv` set to 10.  \n",
    "\n",
    "What this will do is perform [10-fold cross-validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation).  The CV in RidgeCV means that is has this capability built in.  Because this is a Ridge Regressor, it also has regularization built in.  You can check out the [RidgeCV Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html) to see the default values it uses for `alpha` (they are configurable as a parameter of course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "rcv = RidgeCV(cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the following:\n",
    "- train your model on the entire dataset `X` and `y` from above via `fit()` (we can do this because we have built-in cross-validation)\n",
    "- Score the model with `score()`.  You should see a similar score to the one you got with the train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# What about incorporating polynomials?  Ridge regression\n",
    "# Is there an easier way to do train/test/validation and Ridge Regression altogether?  Of course there is!\n",
    "\n",
    "rcv.fit(X, y)\n",
    "rcv.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, awesome!  With just a little variable manipulation we've already improved our baseline model by about 50% and with cross-validation and regularization we have an estimate of our model's future performance that we should feel reasonably comfortable reporting to our boss.\n",
    "\n",
    "Now one last thing to try to squeeze out a little bit more performance.  If you're thinking like a data scientist, you'll probably imagine that the bike traffic by hour will be very different on workdays vs. non-workdays.  Fortunately, we have the data available to test this hypothesis!\n",
    "\n",
    "Let's generate 2 boxplots of Bike Counts by hour with seaborn, the first one for workdays and the 2nd one for non-workdays.  To do this:\n",
    "- Select the rows where workingday==1 from your dataframe, and use that resulting dataframe to build your boxplot as you did many times above\n",
    "- Do the same for workingday==0 for the second plot\n",
    "- Analyze your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "# With just a little consideration of the data we've improved our baseline model about 50%\n",
    "# Let's look just a little further at hours\n",
    "# For when workday = 1 vs 0\n",
    "# Set up the plots\n",
    "fig, axes = plt.subplots(2,1, figsize=(20,10))\n",
    "ax = axes[0]\n",
    "ax.set_title('Counts by Hour on Workdays')\n",
    "# Generate your workingday boxplot here\n",
    "sns.boxplot(x=df[df['workingday']==1]['hr'].sort_values(), y=df['cnt'], ax=ax)\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Count')\n",
    "ax = axes[1]\n",
    "ax.set_title('Counts by Hour on Non-Workdays')\n",
    "# Generate your non-workingday boxplot here\n",
    "sns.boxplot(x=df[df['workingday']==0]['hr'].sort_values(), y=df['cnt'], ax=ax)\n",
    "ax.set_xlabel('Hour')\n",
    "ax.set_ylabel('Count')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What do you notice?\n",
    "You should notice that the 2 distributions are **completely** different.  On non-workdays the middle of the day hours have far more traffic (as one would expect) and the peak commute hours have low traffic.  Thus, it appears that our hypothesis has been shown correct.\n",
    "\n",
    "#### How can we use this?\n",
    "It appears from this that we have 2 fundamentally different distributions of the data.  Perhaps we could use this to split the data along the workingday value and build 2 different models (1 for workdays and 1 for non-workday).  To get a feel for how such a model might do, let's try it on the workdays only (because this distribution looks very similar to the overall one we looked at earlier, we can use the hour indicator variables that we already created for convenience).\n",
    "\n",
    "Let's train a model for workdays only using the same variables we used for our last model and take a look at the results.\n",
    "- Generate the data matrix `X` by selecting the columns `hr_low`, `hr_med`, `hr_high`, `temp`, `hum`, `weathersit`, `windspeed`, `season`, and `workingday`\n",
    "- Select only those rows of `X` where workingday==1 and store this result in `X`\n",
    "- Select the rows of the `cnt` column where workingday==1 and store this result in `y`\n",
    "- Use `train_test_split()` to create a 70/30 split in `X_train`, `X_test`, `y_train`, `y_test`\n",
    "- Create a basic `LinearRegression` model\n",
    "- Call `fit()` to fit your model on the training data\n",
    "- Call `score()` against the test data to view your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are clearly 2 very different distributions!  \n",
    "# Maybe want to consider these 2 situations separately and build a model for each\n",
    "# Let's just take a quick look for workingday = 1\n",
    "X = df.loc[:, ['hr_low', 'hr_med', 'hr_high', 'temp', 'hum', 'weathersit', 'windspeed', 'season', 'workingday']]\n",
    "X = X[X['workingday']==1]\n",
    "y = df[df['workingday']==1]['cnt']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holy smokes we're getting better!\n",
    "If you did it right, you should see another significant improvement.  Hopefully this exercise as shown how the iterative data science pipeline can lead to incrementally better results over time.\n",
    "\n",
    "## What have we done?\n",
    "- Learned how to use statsmodels with R formula syntax for creating linear models\n",
    "- Learned how to evaluate models in different ways (R^2, cross-validation)\n",
    "- Learned some ways to avoid overfitting (cross-validation, regularization)\n",
    "- Understood linear and polynomial regression in sklearn, with cross-validation and regularization\n",
    "- Used seaborn for visualizing relationships in data\n",
    "- Used pandas for manipulating data as we move through our workflow\n",
    "- Gotten a peak into a genuine data science workflow\n",
    "- Seen how curiosity and creativity can yield big gains in a data science modeling pipeline\n",
    "\n",
    "## Play Time\n",
    "See if you can improve the model by trying out whatever methods you like!  Just be sure to cross-validate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pickling our Progress for Later\n",
    "Let's pickle our progress, and put a pin in those thoughts to return to later after we do more learning and stuff.  Save your fitted linear regression model object to its own file, as well as the `X` and `y` Dataframes from the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# STUDENT SECTION\n",
    "import pickle\n",
    "\n",
    "# Save the sklearn model itself\n",
    "\n",
    "    \n",
    "# Save the Dataframes\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
