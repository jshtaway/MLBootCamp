### Plan for Tuesday, May 23

#### Overview

Today we'll cover **Principal Components Analysis (PCA)** and **Singular Value Decomposition (SVD)** for **Dimensionality Reduction**.  This will be invaluable for NLP this week, and your ML modeling in general as you can really extract some powerful features (although it might not be so clear what they mean!).

**Remember:**
* Project McNulty Blogs due **5/23**
* Challenges 10-15 are optional!

#### Schedule

**9:15 am**: Good (Late) Morning

**9:30 am**: Pair Programming:  
  * Pair: [Spam Classification](pair-spam_ham_text.md)   

Pairings:  

| Student 1 | Student 2 |
|---|---|
| Orlando | Sonal |
| Kenny | Walter |
| Alexander | Previous Group |
| Doug | Guo |
| Toni | Andrew |
| Timan | Emily |
| Sathisan | Kailin |
| Daniel | Paul |
| Qingling | Halle |
| Thaddeus | Katie |
| Matt | Florian |

**10:30 am**: Dimensionality Reduction

**12:30 pm**: Lunch

**2:00 pm**: Investigation Presentation

**2:15 pm**: Work

**6:00 pm:** Hasta luego

#### More Resources  
* [Curse of Dimensionality in Classification](http://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/)
* [Curse of Dimensionality (Wikipedia)](http://en.wikipedia.org/wiki/Curse_of_dimensionality)
* [Curse of Dimensionality (Quora)](http://www.quora.com/What-is-the-curse-of-dimensionality)
* Read [A tutorial on Principal Components Analysis](http://www.cs.otago.ac.nz/cosc453/student_tutorials/principal_components.pdf) for a very friendly introduction that starts from the very basics.
* Read the [Stanford PCA Tutorial](http://ufldl.stanford.edu/wiki/index.php/PCA), which is just slightly mathier.
* Read this [step-by-step walk-through](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html) of PCA with Python.
* Read an excellent paper on [The Fundamental Theorem of Linear Algebra](http://home.eng.iastate.edu/~julied/classes/CE570/Notes/strangpaper.pdf) which goes through in an intuitive way just what SVD is all about.
* Watch the [Chapter 10 lecture videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/) from *An Introduction to Statistical Learning* on PCA.
* [PCA and dimensionality reduction 4 dummies](http://georgemdallas.wordpress.com/2013/10/30/principal-component-analysis-4-dummies-eigenvectors-eigenvalues-and-dimension-reduction/)
* [Step by step PCA math in Python](http://sebastianraschka.com/Articles/2014_pca_step_by_step.html)
* Learn about [random projections](http://users.ics.aalto.fi/ella/publications/randproj_kdd.pdf) and try them [in sci-kit](http://scikit-learn.org/stable/modules/random_projection.html).
* Learn about t-Distributed Stochastic Neighbor Embedding ([t-SNE](http://homepage.tudelft.nl/19j49/t-SNE.html)), another technique that can be great for making complex data visualizable. It helped [win](http://blog.kaggle.com/2012/11/02/t-distributed-stochastic-neighbor-embedding-wins-merck-viz-challenge/) a Kaggle visualization contest, for example.
