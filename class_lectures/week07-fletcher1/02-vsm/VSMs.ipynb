{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "init_cell": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "# Basics\n",
    "from __future__ import print_function, division\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "# gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "# sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics.pairwise as smp\n",
    "\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# logging for gensim (set to INFO)\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vector Space Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Yesterday, we used [**Latent Dirichlet Allocation (LDA)**](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) in [gensim](http://radimrehurek.com/gensim/index.html) to map text documents ([20 Newsgroups dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)) from a **word space** to a **topic space** that could give us the **topic distribution** of different documents in our corpus as well as allow us to make **conceptual comparisons** of documents in the reduced topic space.\n",
    "\n",
    "Today, we'll continue mapping text documents from a highly dimensional word (or token) space into a much reduced **semantic space** which allows us to make valuable **conceptual comparisons** between arbitrary blocks of text in this new vector space.\n",
    "\n",
    "Thus the **input is a large corpus of text documents** and the **output is a reduced semantic space for those input documents and words**.  These starting/ending points are constant, but we'll take 2 different approaches for the process in between:\n",
    "1.  [**Latent Semantic Indexing (LSI)**](https://en.wikipedia.org/wiki/Latent_semantic_analysis) - performs a [**Singular Value Decomposition (SVD)**](https://en.wikipedia.org/wiki/Singular_value_decomposition) on a [**document-term matrix**](https://en.wikipedia.org/wiki/Document-term_matrix) with [**TFIDF Weightings**](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) to map all the terms in the corpus into a reduced **term space** and all the documents into a reduced **document space**.  \n",
    "    - The 2 spaces are related by a simple transformation, so we can perform arbitrary **term-term**, **doc-doc**, and **doc-term comparisons** via [**cosine similarity**](https://en.wikipedia.org/wiki/Cosine_similarity).\n",
    "    - These 2 spaces make up the **\"dual space\"**\n",
    "        - Every document is the weighted sum of all of its terms\n",
    "        - Every term is the weighted sum of all the documents it occurs in (very useful!)\n",
    "2. [**Non-Negative Matrix Factorization (NMF)**](https://en.wikipedia.org/wiki/Non-negative_matrix_factorization) - performs a different type of matrix factorization on the **Term-Document Matrix** to yield reduced Term/Document vector spaces.  \n",
    "  - Yields **word vectors** just like all our methods today.\n",
    "    - These vectors are often quite similar to LSI vectors.\n",
    "  - Yields **document vectors** as well.\n",
    "  - Decomposition has some nice properties.\n",
    "3.  [**Word2Vec**](https://en.wikipedia.org/wiki/Word2vec) - uses a neural network to yield **term space**\n",
    "    - Has additional nice properties of term vectors, such as conceptual additivity (see below)\n",
    "\n",
    "## Goals &zwnj;\n",
    "- Continue to use gensim to implement text modeling\n",
    "- Build an LSI vector space from a training set\n",
    "- Use the LSI space to compare terms and documents to one another conceptually\n",
    "- Use the LSI space to perform document clustering and classification\n",
    "- Build an NMF vector space from a training set\n",
    "- Use the NMF space to compare terms and documents to one another conceptually\n",
    "- Use the NFM space to perform document clustering and classification\n",
    "- Use Word2vec to create a vector space for words in a training set\n",
    "- Use the Word2vec space to do simple comparisons between different combinations of words\n",
    "- Discuss various other considerations, tasks, and extensions for VSMs like LSI, NMF, and Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "- Vector Space Models\n",
    "  - What?\n",
    "  - Why?\n",
    "  - How?\n",
    "- TFIDF\n",
    "- Latent Semantic Indexing\n",
    "- Non-negative Matrix Factorization\n",
    "- Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are Vector Space Models?\n",
    "- Map raw text into vector space\n",
    "- Allow semantic (conceptual) comparison between text chunks\n",
    "- **Input**: Corpus of raw text documents\n",
    "- **Output**: Vectors for text documents (and usually terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need Vector Space Models?\n",
    "- Early NLP (1950s-1980s): focused on **linguistics** and **handwritten rules**\n",
    "  - Extremely complex, impossible to maintain/scale\n",
    "- 1980s: ML introduced for NLP\n",
    "  - Linguistics $\\rightarrow$ \"***Corpus Linguistics***\"\n",
    "  - Learn from text via large ***corpora*** of labeled raw text documents\n",
    "- **Idea**: Map text to mathematical entities $\\rightarrow$ ***Word Vectors***\n",
    "- No more complex rules systems! $\\rightarrow$ *Unsupervised*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do VSMs work?\n",
    "- Start with raw text\n",
    "- Translate text to vectors (e.g. Counts, TFIDF)\n",
    "- Optional (Probable): **Reduce the space**\n",
    "  - Use Probabilistic Inference (LDA)\n",
    "  - Use Dimensionality Reduction via **Matrix Factorization** (SVD, NMF)\n",
    "  - Use a Neural Network (Word2Vec)\n",
    "- Once we have \"Semantic\" (meaning) vectors (**word vectors**):\n",
    "  - Do all sorts of ML with them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of VSMs\n",
    "- **LDA**:\n",
    "  - Word Space $\\rightarrow$ Topic Space via Bayesian Inference\n",
    "- **TFIDF**:\n",
    "  - Word Space not reduced, but augmented\n",
    "- **LSA/LSI** and **NMF**:\n",
    "  - Word Space $\\rightarrow$ Semantic Space via SVD or NMF \n",
    "- **Word2Vec**:\n",
    "  - Word Space $\\rightarrow$ Semantic Space via Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Term Frequency Inverse Document Frequency (TFIDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TFIDF\n",
    "- Creates vectors for documents based on unique term counts (frequencies)\n",
    "- Weights the frequency counts by TFIDF weighting\n",
    "- Does **not** reduce dimensionality\n",
    "- Frequent preprocessing step for matrix factorization methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Term-Document Matrix (TDM)\n",
    "- Start with corpus of documents (raw text)\n",
    "- Create matrix:\n",
    "  - Rows are our **term vocabulary** - unique terms over all documents\n",
    "  - Columns are all documents\n",
    "  - Entries are respective term frequency for each document\n",
    "- \"Terms\" means tokens, whatever is extracted from tokenization in preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TFIDF Weighting\n",
    "- Term Frequency Inverse Document Frequency\n",
    "- Common weighting scheme applied to term-document matrix\n",
    "- Any function that is:\n",
    "  - Directly proportional to term frequency **within document** (local weight)\n",
    "  - Inversely proportional to term frequency **in all documents** (global weight)\n",
    "- **Motivation**: Highly common terms are not useful to distinguish documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TFIDF: Relation to VSMs\n",
    "- TFIDF vectors are a VSM on their own! \n",
    "- TFIDF weightings empirically improve VSMs\n",
    "- Some VSMs (LSI, NMF) almost certainly require this step, some don't (LDA, Word2Vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing Considerations\n",
    "- Tokenization: Determines our unique terms $\\rightarrow$ size of matrix\n",
    "- Stopwords\n",
    "- Stemming\n",
    "- Named Entity Recognition\n",
    "- Punctuation\n",
    "- Min/Max Frequency Threshold (how often should term occur to keep it?)\n",
    "- Phrase Extraction\n",
    "- Part-of-Speech Tagging\n",
    "- Word-sense Disambiguation (bush vs George Bush)\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common TFIDF Weighting Scheme\n",
    "- **Entropy**:\n",
    "  - Term Frequency for term $i$ in document $j$: $tf_{ij}$\n",
    "  - Term Frequency for term $i$ over all documents: $gf_i$\n",
    "  - Relative Frequency for term $i$ in document $j$: $p_{ij} = tf_{ij}/gf_i$\n",
    "  - Number of documents in corpus: $n$\n",
    "  - Local Weight (\"TF\"): $lw_{ij} = \\log(tf_{ij}+1)$\n",
    "  - Global Weight (\"IDF\"): $gw_i = 1 + \\sum\\limits_j \\frac{p_{ij}\\log p_{ij}}{\\log n}$\n",
    "  - TFIDF Weight: \n",
    "$$\n",
    "tfidf = lw_{ij}\\times gw_i = \\left(\\log(tf_{ij}+1)\\right) \\times \\left(1 + \\sum\\limits_j \\frac{p_{ij}\\log p_{ij}}{\\log n}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**TFIDF in `sklearn`**: \n",
    "Let's implement TFIDF with the [20 Newsgroups Dataset](http://scikit-learn.org/stable/datasets/twenty_newsgroups.html)\n",
    "- Here's TFIDF in `sklearn` with `TfidfVectorizer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load in the 20 Newsgroups data\n",
    "ng = datasets.fetch_20newsgroups()\n",
    "# Get the raw text docs\n",
    "ng_text = ng.data\n",
    "\n",
    "# Vectorize the text using TFIDF\n",
    "tfidf = TfidfVectorizer(stop_words=\"english\", \n",
    "                        token_pattern=\"\\\\b[a-zA-Z][a-zA-Z]+\\\\b\", \n",
    "                        min_df=10)\n",
    "tfidf_vecs = tfidf.fit_transform(ng_text)\n",
    "pd.DataFrame(tfidf_vecs.todense(), \n",
    "             columns=tfidf.get_feature_names()\n",
    "            ).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Applications of TFIDF\n",
    "- We have a vector space!\n",
    "  - Documen vectors are the rows\n",
    "  - Term vectors are the columns\n",
    "- We can try to do ML\n",
    "  - e.g.: Naive Bayes for Text Classification\n",
    "- **BUT**...\n",
    "  - Many unique terms $\\rightarrow$ many unique features!\n",
    "  - Curse of Dimensionality :(\n",
    "  - Dimensionality Reduction seems prudent :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Naive Bayes Text Classification\n",
    "- Naive Bayes empirically avoids the curse somewhat on TFIDF vectors\n",
    "  - Performs decent enough on high-dimensional text classification tasks\n",
    "- How does it work?\n",
    "  - The observations are the weighted TFIDF vectors\n",
    "    - This is a (weighted) bag of words for document $j$ and terms $\\{w_i\\}$\n",
    "$$\n",
    "P(Class C | \\{w_i\\}) = \\frac{(\\text{Likelihood} \\times \\text{Prior}}{\\text{Evidence}} = \\frac{P(\\{w_i\\} | C) \\times P(C)}{P(\\{w_i\\})}\n",
    "$$\n",
    "- Naive Assumption: \n",
    "$$\n",
    "P(\\{w_i\\} | C) = \\prod\\limits_i P(w_i|C)\n",
    "$$\n",
    "- This is just a multinomial distribution where **given a class C each word has probability $p_i$ of appearing**!\n",
    "- Thus:\n",
    "  - **Likelihood is multinomial**:\n",
    "    - $P(w_i)|C)$: Number of times word $w_i$ appears in class C documents divided by total number of words in Class C documents\n",
    "  - **Prior**: Proportion of documents of class C\n",
    "  - We can use Multinomial Naive Bayes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try simple Naive Bayes classification on TFIDF vectors from above in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "marked": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_vecs, \n",
    "                                                    ng.target, \n",
    "                                                    test_size=0.33)\n",
    "\n",
    "# Train \n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Test \n",
    "nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Latent Semantic Analysis/Indexing (LSA/LSI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is LSI?\n",
    "- Latent Semantic Indexing (Analysis $\\rightarrow$ LSA, same thing)\n",
    "- **TFIDF space** $\\rightarrow$ **Semantic space**\n",
    "  - **Dimensionality Reduction**!\n",
    "- **SVD** performs the reduction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steps in LSI\n",
    "- Create Term-Document Matrix\n",
    "- Apply TFIDF Weightings\n",
    "- Perform SVD on TFIDF Matrix\n",
    "  - Results in **Term space** and **Document space**\n",
    "  - Spaces linked by simple transformation\n",
    "- Keep the top $k$ components (dimensions, this is a parameter $\\rightarrow$ your choice!)\n",
    "- Use the resulting vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SVD for LSI\n",
    "<img  src=\"svd.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSI Preprocessing and Other Considerations\n",
    "- Everything from TFIDF\n",
    "- Often remove numerics, highly infrequent terms, etc\n",
    "- Named Entity Recognition (NER)\n",
    "  - Tagging entities dramatically improves results for some data\n",
    "- Bottlenecks\n",
    "  - TDM creation (parallelizable)\n",
    "  - SVD (good approximation algorithms for **incremental SVD**)\n",
    "- Do I even need to train on my data?\n",
    "  - If you have pre-existing word vectors from similar domain, maybe not!  Use those!\n",
    "- Do I need to train on all of my data?\n",
    "  - Maybe as little as 10% is needed for **large** datasets (tens of millions of docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSI Parameters\n",
    "- Number of dimensions ($k$) to reduce to:\n",
    "  - Depends on data size\n",
    "  - 300 old standard, 500-1000 probably better for large datasets\n",
    "  - 100 or less is fine for **small** data\n",
    "- TFIDF Weightings:\n",
    "  - Entropy most successful empirically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Results of LSI\n",
    "- **Output**: Word (semantic) vectors!\n",
    "- With these we can:\n",
    "  - Make conceptual term-term, term-doc, doc-doc, comparisons via **Cosine Similarity**\n",
    "  - Perform other ML tasks:\n",
    "    - Classification\n",
    "    - Clustering\n",
    "    - Regression (less common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSI with `gensim`\n",
    "- Let's try out LSI with `gensim`!\n",
    "- First we need to export our TFIDF vectors (from earlier) to `gensim` and let it know the mapping of row index to term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert sparse matrix of counts to a gensim corpus\n",
    "# Need to transpose it for gensim which wants \n",
    "# terms by docs instead of docs by terms\n",
    "tfidf_corpus = matutils.Sparse2Corpus(tfidf_vecs.transpose())\n",
    "\n",
    "# Row indices\n",
    "id2word = dict((v, k) for k, v in tfidf.vocabulary_.items())\n",
    "\n",
    "# This is a hack for Python 3!\n",
    "id2word = corpora.Dictionary.from_corpus(tfidf_corpus, \n",
    "                                         id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Now let's build an LSI space!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Build an LSI space from the input TFIDF matrix, mapping of row id to word, and num_topics\n",
    "# num_topics is the number of dimensions to reduce to after the SVD\n",
    "# Analagous to \"fit\" in sklearn, it primes an LSI space\n",
    "lsi = models.LsiModel(tfidf_corpus, id2word=id2word, num_topics=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using the LSI Space in `gensim`\n",
    "- We have a trained LSI space\n",
    "- We want to see where original documents lie in that 300-dimensional space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve vectors for the original tfidf corpus in the LSI space (\"transform\" in sklearn)\n",
    "lsi_corpus = lsi[tfidf_corpus]\n",
    "\n",
    "# Dump the resulting document vectors into a list so we can take a look\n",
    "doc_vecs = [doc for doc in lsi_corpus]\n",
    "doc_vecs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Conceptual Similarity Between Documents\n",
    "- Compare any document in the space to any other\n",
    "- Cosine Similarity:\n",
    "$$\n",
    "\\text{sim} = \\frac{x_1 \\cdot x_2}{\\lVert{x_1}\\rVert \\lVert{x_2}\\rVert}\n",
    "$$\n",
    "- In `gensim`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create an index transformer that calculates similarity based on \n",
    "# our space\n",
    "index = similarities.MatrixSimilarity(doc_vecs, \n",
    "                                      num_features=len(id2word))\n",
    "\n",
    "# Return the sorted list of cosine similarities to the first document\n",
    "sims = sorted(enumerate(index[doc_vecs[0]]), key=lambda item: -item[1])\n",
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's take a look at how we did\n",
    "for sim_doc_id, sim_score in sims[0:3]: \n",
    "    print(\"Score: \" + str(sim_score))\n",
    "    print(\"Document: \" + ng_text[sim_doc_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conceptual Similarity Between Arbitrary Text Blobs\n",
    "- We have vectors in the LSI space\n",
    "- We can compare any blob of text to any other blob of text!  \n",
    "- How?\n",
    "  - Need to send any blob of text through our entire preprocessing + LSI transformation pipeline\n",
    "  - So `gensim` can index them and perform the comparisons.  \n",
    "\n",
    "Let's create some text for a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create some test text blobs to compare pairwise\n",
    "text_blobs = ['space', 'nasa', 'science', 'armenians', 'israel', \n",
    "              'space nasa program', 'turkish middle east', \n",
    "              'computer graphics', 'computers', 'data science']\n",
    "print(text_blobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conceptual Similarity Between Arbitrary Text Blobs\n",
    "Now we need to do our transformations.  Following what we did above, the are:\n",
    "* Use `TFIDF` from above to get tfidf vectors\n",
    "* Convert the numpy array to a gensim corpus\n",
    "* Perform LSI transformation from above on the corpus\n",
    "\n",
    "Should build a function for all this, but let's just try them sequentially here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Get tfidf matrix\n",
    "test_vecs = tfidf.transform(text_blobs).transpose()\n",
    "# Convert to gensim corpus\n",
    "test_corpus = matutils.Sparse2Corpus(test_vecs)\n",
    "# LSI transformation\n",
    "test_lsi = lsi[test_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conceptual Similarity Between Arbitrary Text Blobs\n",
    "- We have LSI vectors for all of our test \"documents\" (text blobs)!  \n",
    "- We just need to index them\n",
    "- Then we can compare them to one another via cosine similarity.  \n",
    "- To index them we use the `MatrixSimilarity` that we did above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Index our test text blobs\n",
    "test_index = similarities.MatrixSimilarity(test_lsi)\n",
    "\n",
    "# Iterate and print out all pairwise similarities\n",
    "# For each test text blob that we're looking at\n",
    "for i, sims in enumerate(test_index):\n",
    "    # We get a list of similarities to all indexed text blobs\n",
    "    # Print the text blob we're currently examining\n",
    "    print(\"Similarities to {}:\".format(text_blobs[i]))\n",
    "    # Print the similarities of the current blob to all others with labels\n",
    "    sims_with_labels = [(score, text_blobs[j]) for j, score in enumerate(sims)]\n",
    "    # Sort the results by decreasing similarity and print them out\n",
    "    sorted_sims_with_labels = sorted(sims_with_labels, reverse=True)\n",
    "    print(sorted_sims_with_labels)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- So cool!!  \n",
    "- We can compare ***any*** arbitrary collection of words (arbitrary \"documents\") to any other arbitrary collection of words with our gensim LSI index.  \n",
    "- We just need to make sure we index those documents first!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSI for Machine Learning\n",
    "- We have (very good, 300-dimensional) vectors for our documents now!  \n",
    "- So we can do any ML we want on our documents!\n",
    "- First we need to convert back to `sklearn` land:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the gensim-style corpus vecs to a numpy array for sklearn manipulations\n",
    "ng_lsi = matutils.corpus2dense(lsi_corpus, num_terms=300).transpose()\n",
    "ng_lsi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### LSI for Text Clustering\n",
    "- Let's try clustering our documents with `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "marked": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create KMeans\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Cluster\n",
    "ng_lsi_clusters = kmeans.fit_predict(ng_lsi)\n",
    "\n",
    "# Take a look\n",
    "print(ng_lsi_clusters[0:50])\n",
    "ng_text[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### LSI for Text Classification\n",
    "- Try some simple classification on the result LSI vectors for the 20 NG set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: This is very expensive, don't run in-class\n",
    "# Need pairwise Cosine for KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics.pairwise as smp\n",
    "\n",
    "# Train/Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(ng_lsi, ng.target, \n",
    "                                                    test_size=0.33)\n",
    "\n",
    "# Fit KNN classifier to training set with cosine distance\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-Negative Matrix Factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is NMF?\n",
    "- Non-negative Matrix Factorization\n",
    "- **TFIDF space** $\\rightarrow$ **Semantic space**\n",
    "  - **Dimensionality Reduction**!\n",
    "  - Just like LSI in that way!\n",
    "- Only difference: Different Matrix Factorization\n",
    "  - LSI is SVD:\n",
    "$$\n",
    "\\text{X} = \\text{U}\\Sigma\\text{V}^T\n",
    "$$\n",
    "  - NMF:\n",
    "$$\n",
    "\\text{X} = \\text{TD}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NMF Factorization\n",
    "$$\n",
    "\\text{X} = \\text{TD}\n",
    "$$\n",
    "- X: Term-Document Space (familiar TFIDF matrix)\n",
    "- T: Term-Feature Space (familiar reduced term space, though not the same one!)\n",
    "- D: Document-Feature Space (familiar reduced doc space, though not the same one!)\n",
    "- NMF Vectors are often very similar to LSI vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NMF Factorization\n",
    "<img src='NMF.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Steps in NMF for Text\n",
    "- Same as LSI!\n",
    "  - Create Term-Document Matrix\n",
    "  - Apply TFIDF weightings\n",
    "  - Factorize Matrix (this time with NMF instead of SVD)\n",
    "  - Use resulting \"semantic\" vectors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NMF for ML\n",
    "- Just like with LSI, can use NMF vectors for ML\n",
    "- `sklearn` has NMF, `gensim` does not\n",
    "- We'll reuse the TFIDF and data pieces from before\n",
    "- Let's reduce the TFIDF matrix in `sklearn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Reduce TFIDF Matrix to 300 dimensions\n",
    "from sklearn.decomposition import NMF\n",
    "nmf = NMF(n_components=300)\n",
    "nmf_vecs = nmf.fit_transform(tfidf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NMF for Text Clustering\n",
    "- Now let's cluster the docs in NMF space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "marked": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# KMeans clustering on Newsgroups\n",
    "# Use the data loaded from earlier\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=20)\n",
    "kmeans.fit_predict(nmf_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### NMF for Text Classification\n",
    "- And now classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# WARNING: Expensive, do not run now\n",
    "# Fit KNN classifier to training set with cosine distance\n",
    "# Use our train/test split from LSI example\n",
    "knn = KNeighborsClassifier(n_neighbors=3, metric=smp.cosine_distances)\n",
    "knn.fit(X_train, y_train)\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Woooohoooo!  We've classified text with NMF!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is word2vec?\n",
    "- VSM for text analysis\n",
    "- **Input**: Corpus of text documents\n",
    "- **Output**: Reduced vector space for all terms in documents\n",
    "- **Training**: Neural Network based on sliding windows of text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why word2vec?\n",
    "- Other VSMs (LSI, NMF, etc) **don't capture word order** (Bag of Words)!\n",
    "- It would be nice if we could, at least a little!\n",
    "- word2vec does somewhat\n",
    "- word2vec can capture analogy relationships:\n",
    "  - e.g.: King is to man as Queen is to woman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How does word2vec work?\n",
    "- We use a **shallow** (1 hidden layer) neural network \n",
    "- Train on \"**context windows**\", small sequences of words in text (5-10 maybe)\n",
    "- The input is either:\n",
    "  - A word in the context window (\"Skip-Grams\")\n",
    "  - All words but 1 in a context window (\"CBOW\")\n",
    "- The output is either:\n",
    "  - All the other words in a context window (\"Skip-Grams\")\n",
    "  - The missing word from the context window (\"CBOW\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Preprocessing Considerations\n",
    "- Everything from our previous VSMs!\n",
    "- aka Tokenization, stemming, stopwords, Entity extraction, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training word2Vec\n",
    "- Use a neural network on context windows\n",
    "- 2 main approaches for inputs and labels:\n",
    "  - **Skip-Grams**\n",
    "  - **Continuous Bag of Words (CBOW)**\n",
    "  - Vectors usually similar, subtle differences, also differences in computational time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Context Windows\n",
    "- **Observations** for word2vec: All **context windows** in a corpus\n",
    "  - Size of a context window is a chosen parameter\n",
    "- e.g.:\n",
    "  - Document: \"The quick brown fox jumped over the lazy dog.\"\n",
    "  - Window size: 5\n",
    "  - Window 1: \"`The quick brown fox jumped` over the lazy dog.\"\n",
    "  - Window 2: \"The `quick brown fox jumped over` the lazy dog.\"\n",
    "  - Window 3: \"The quick `brown fox jumped over the` lazy dog.\"\n",
    "  - Window 4: \"The quick brown `fox jumped over the lazy` dog.\"\n",
    "  - Window 5: \"The quick brown fox `jumped over the lazy dog`.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### \"One-hot\" Encoding Word Vectors\n",
    "- We need to be able to represent a **sequence of words** as a vector\n",
    "- To do this, we need to assign each word an index from 0 to V\n",
    "  - V is the size of the vocabulary aka # distinct words in the corpus\n",
    "- A **word vector** is:\n",
    "  - 1 for the index of that word\n",
    "  - 0 for all other entries  \n",
    "<img src='1-hot.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One-hot Encoding Context Windows\n",
    "- We need vectors for context windows\n",
    "- A sequence of words will have a vector that's just the concatenation of its word vectors\n",
    "  - Thus, for window size $d$ the vector is of length $V \\times d$\n",
    "  - Only $d$ entries (one for each word) will be nonzero (1s)\n",
    "  \n",
    "<img src='catdog.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Skip-Grams\n",
    "- Build a neural network with 1 hidden layer\n",
    "- **Inputs**: \n",
    "  - The middle word of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V$\n",
    "- **Outputs**: \n",
    "  - The other words of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V \\times (d-1)$\n",
    "- Turn the crank!  \n",
    "<img src='skip_gram.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Continuous Bag of Words (CBOW)\n",
    "- Build a neural network with 1 hidden layer\n",
    "- Just reverse of Skip-Grams!\n",
    "- **Inputs**: \n",
    "  - The other words of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V \\times (d-1)$\n",
    "- **Outputs**: \n",
    "  - The middle word of the context window (one-hot encoded)\n",
    "  - Dimensionality: $V$\n",
    "- Turn the crank!  \n",
    "<img src='cbow.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Dimensionality Reduction\n",
    "- Number of nodes in hidden layer, $N$, is a parameter\n",
    "- It is the (reduced) dimensionality of our resulting word vector space!\n",
    "- Fit the neural net $\\rightarrow$ find weights matrix $W$\n",
    "  - In the new space, $x_N = W^Tx$\n",
    "  - Checking dimensions:\n",
    "    - $x$: $V \\times 1$\n",
    "    - $W^T$: $N \\times V$\n",
    "    - $x_N$: $N \\times 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### So What is Happening?\n",
    "- We're learning the words likely to appear near each word\n",
    "- This context information ultimately leads to vectors for related words falling near one another!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nice Properties of Word2Vec Vectors\n",
    "- word2vec (somewhat magically!) captures nice geometric relations between words\n",
    "<img src='vector_queen2.png' align='right'/>\n",
    "- e.g.: Analogies\n",
    "  - King is to Queen as man is to woman\n",
    "  - The vector between King and Queen is the same as that between man and woman!\n",
    "- Works for all sorts of things: capitals, cities, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Word2Vec for ML\n",
    "- Again we get **word vectors**!\n",
    "- So we can use them for ML!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using Existing Word Vectors\n",
    "- word2vec takes **A LOT** of data to train it well\n",
    "- What if you don't have **A LOT** of data?\n",
    "  - Steal someone else's vectors!\n",
    "  - Google has trained a [giant set](https://code.google.com/p/word2vec/)\n",
    "  - So has [Stanford NLP](http://nlp.stanford.edu/projects/glove/) (slightly different, same idea)\n",
    "  - Many others\n",
    "- As long as the domain is similar, should be better than yours\n",
    "  - Need words to have the same meaning as in your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### word2vec in `gensim`\n",
    "- Very simple example for usage\n",
    "- We'll train our own (don't!  steal vectors!)\n",
    "- It needs a list of lists representing the sentences in a corpus\n",
    "- Here's how that goes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Make sure gensim and Word2Vec are installed and functional\n",
    "# Create some dummy data\n",
    "sentences = documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",\n",
    "              \"Relation of user perceived response time to error measurement\",\n",
    "              \"The generation of random binary unordered trees\",\n",
    "              \"The intersection graph of paths in trees\",\n",
    "              \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "              \"Graph minors A survey\"]\n",
    "\n",
    "# The type of input that Word2Vec is looking for.. \n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in documents]\n",
    "\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Train word2vec\n",
    "w2v = models.Word2Vec(texts, size=100, window=5, min_count=1, workers=4,sg=1)\n",
    "# Check out the resulting vector for \"computer\"\n",
    "w2v['computer']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Now on a slightly Larger Corpus**:  \n",
    "- Using Project Gutenberg Corpus (Books) from NLTK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# An Illustration.. \n",
    "\n",
    "import os\n",
    "\n",
    "# Create an iterator Class that can be used as a gensim corpus (defines how to read in the text data)\n",
    "class MySentences(object):\n",
    "     def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    " \n",
    "     def __iter__(self):\n",
    "         for fname in os.listdir(self.dirname):\n",
    "                for line in open(\n",
    "                    os.path.join(self.dirname, fname), \n",
    "                    encoding='utf-8', errors='ignore'):\n",
    "                    yield line.split()\n",
    "\n",
    "# Instantiate the corpus from a text file of documents\n",
    "# You'll need to change the path!\n",
    "sentences = MySentences('/Users/paulburkard//nltk_data/corpora/gutenberg') # a memory-friendly iterator\n",
    "# Create a Word2vec model\n",
    "w2v = models.Word2Vec(sentences,min_count=3,workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Calculating Similarities\n",
    "- Have word vectors for all terms!  \n",
    "- Gensim provides a number of methods to then do comparisons in this vector space \n",
    "- Here are a few:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Words close to woman and king but not man\n",
    "w2v.most_similar(positive=['woman', 'king'], negative=['man'], topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Not so hot!**  \n",
    "- Unsurprising: Need much more data\n",
    "- A lesson in why you should steal vectors :)\n",
    "- Let's try some pairwise comparisons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Similarity between woman and man\n",
    "print(w2v.similarity('woman','man'))\n",
    "\n",
    "# Similarity between bags of words\n",
    "print(w2v.n_similarity(['woman', 'girl'], ['man', 'boy']))\n",
    "\n",
    "# Finding words that don't match others in a bag\n",
    "print(w2v.doesnt_match(\"breakfast man dinner lunch\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2Vec for Text Clustering\n",
    "- We won't do it here, but the procedure is exactly the same as LSI above:\n",
    "  - Export the vectors back to `sklearn`\n",
    "  - Try whatever clustering algorithm you like on the reduced vectors\n",
    "- **OR** more likely you just take the vectors as given from Google or Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Word2Vec for Text Classification\n",
    "- We won't do it here, but the procedure is exactly the same as LSI above:\n",
    "  - Export the vectors back to `sklearn`\n",
    "  - Try whatever classifying algorithm you like on the reduced vectors\n",
    "    - Almost certainly KNN with Cosine Similarity\n",
    "- **OR** more likely you just take the vectors as given from Google or Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "frozen": false,
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications of Word Vectors (VSMs) \n",
    "- With word vectors, we can do so many cool things:\n",
    "  - ML algorithms\n",
    "  - Machine Translation\n",
    "  - Many of those things I mentioned on NLP day 1\n",
    "  - Seed Deep Learning with them to do **even cooler stuff**\n",
    "- Basically, we know the state of the world (the meaning of words)...\n",
    "  - The possibilities are endless!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "livereveal": {
   "height": "100%",
   "margin": 0,
   "maxScale": 1,
   "minScale": 1,
   "scroll": true,
   "start_slideshow_at": "selected",
   "theme": "metis",
   "transition": "zoom",
   "width": "100%"
  },
  "toc": {
   "nav_menu": {
    "height": "369px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "457px",
    "left": "0px",
    "right": "968px",
    "top": "130px",
    "width": "214px"
   },
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
